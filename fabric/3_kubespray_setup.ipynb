{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca4dada6-bc00-4743-a90c-f0a6460d10ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7b1e7 tr:nth-child(even) {\n",
       "  background: #dbf3ff;\n",
       "  color: #231f20;\n",
       "}\n",
       "#T_7b1e7 tr:nth-child(odd) {\n",
       "  background: #ffffff;\n",
       "  color: #231f20;\n",
       "}\n",
       "#T_7b1e7 caption {\n",
       "  text-align: center;\n",
       "  font-size: 150%;\n",
       "}\n",
       "#T_7b1e7_row0_col0, #T_7b1e7_row0_col1, #T_7b1e7_row1_col0, #T_7b1e7_row1_col1, #T_7b1e7_row2_col0, #T_7b1e7_row2_col1, #T_7b1e7_row3_col0, #T_7b1e7_row3_col1, #T_7b1e7_row4_col0, #T_7b1e7_row4_col1, #T_7b1e7_row5_col0, #T_7b1e7_row5_col1, #T_7b1e7_row6_col0, #T_7b1e7_row6_col1, #T_7b1e7_row7_col0, #T_7b1e7_row7_col1, #T_7b1e7_row8_col0, #T_7b1e7_row8_col1, #T_7b1e7_row9_col0, #T_7b1e7_row9_col1, #T_7b1e7_row10_col0, #T_7b1e7_row10_col1, #T_7b1e7_row11_col0, #T_7b1e7_row11_col1, #T_7b1e7_row12_col0, #T_7b1e7_row12_col1, #T_7b1e7_row13_col0, #T_7b1e7_row13_col1, #T_7b1e7_row14_col0, #T_7b1e7_row14_col1, #T_7b1e7_row15_col0, #T_7b1e7_row15_col1, #T_7b1e7_row16_col0, #T_7b1e7_row16_col1, #T_7b1e7_row17_col0, #T_7b1e7_row17_col1, #T_7b1e7_row18_col0, #T_7b1e7_row18_col1 {\n",
       "  text-align: left;\n",
       "  border: 1px #231f20 solid !important;\n",
       "  overwrite: False;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7b1e7\">\n",
       "  <caption>FABlib Config</caption>\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_7b1e7_row0_col0\" class=\"data row0 col0\" >Orchestrator</td>\n",
       "      <td id=\"T_7b1e7_row0_col1\" class=\"data row0 col1\" >orchestrator.fabric-testbed.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7b1e7_row1_col0\" class=\"data row1 col0\" >Credential Manager</td>\n",
       "      <td id=\"T_7b1e7_row1_col1\" class=\"data row1 col1\" >cm.fabric-testbed.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7b1e7_row2_col0\" class=\"data row2 col0\" >Core API</td>\n",
       "      <td id=\"T_7b1e7_row2_col1\" class=\"data row2 col1\" >uis.fabric-testbed.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7b1e7_row3_col0\" class=\"data row3 col0\" >Artifact Manager</td>\n",
       "      <td id=\"T_7b1e7_row3_col1\" class=\"data row3 col1\" >artifacts.fabric-testbed.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7b1e7_row4_col0\" class=\"data row4 col0\" >Token File</td>\n",
       "      <td id=\"T_7b1e7_row4_col1\" class=\"data row4 col1\" >/home/fabric/.tokens.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7b1e7_row5_col0\" class=\"data row5 col0\" >Project ID</td>\n",
       "      <td id=\"T_7b1e7_row5_col1\" class=\"data row5 col1\" >49f65ad7-d8a2-4ab9-8ca0-ba777a2e0ea2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7b1e7_row6_col0\" class=\"data row6 col0\" >Bastion Host</td>\n",
       "      <td id=\"T_7b1e7_row6_col1\" class=\"data row6 col1\" >bastion.fabric-testbed.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7b1e7_row7_col0\" class=\"data row7 col0\" >Bastion Username</td>\n",
       "      <td id=\"T_7b1e7_row7_col1\" class=\"data row7 col1\" >jakejongejans_0000313927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7b1e7_row8_col0\" class=\"data row8 col0\" >Bastion Private Key File</td>\n",
       "      <td id=\"T_7b1e7_row8_col1\" class=\"data row8 col1\" >/home/fabric/work/fabric_config/fabric_bastion_key</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7b1e7_row9_col0\" class=\"data row9 col0\" >Slice Public Key File</td>\n",
       "      <td id=\"T_7b1e7_row9_col1\" class=\"data row9 col1\" >/home/fabric/work/fabric_config/slice_key.pub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7b1e7_row10_col0\" class=\"data row10 col0\" >Slice Private Key File</td>\n",
       "      <td id=\"T_7b1e7_row10_col1\" class=\"data row10 col1\" >/home/fabric/work/fabric_config/slice_key</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7b1e7_row11_col0\" class=\"data row11 col0\" >Sites to avoid</td>\n",
       "      <td id=\"T_7b1e7_row11_col1\" class=\"data row11 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7b1e7_row12_col0\" class=\"data row12 col0\" >SSH Command Line</td>\n",
       "      <td id=\"T_7b1e7_row12_col1\" class=\"data row12 col1\" >ssh -i {{ _self_.private_ssh_key_file }} -F /home/fabric/work/fabric_config/ssh_config {{ _self_.username }}@{{ _self_.management_ip }}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7b1e7_row13_col0\" class=\"data row13 col0\" >Log Level</td>\n",
       "      <td id=\"T_7b1e7_row13_col1\" class=\"data row13 col1\" >INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7b1e7_row14_col0\" class=\"data row14 col0\" >Log File</td>\n",
       "      <td id=\"T_7b1e7_row14_col1\" class=\"data row14 col1\" >/tmp/fablib/fablib.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7b1e7_row15_col0\" class=\"data row15 col0\" >Bastion SSH Config File</td>\n",
       "      <td id=\"T_7b1e7_row15_col1\" class=\"data row15 col1\" >/home/fabric/work/fabric_config/ssh_config</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7b1e7_row16_col0\" class=\"data row16 col0\" >Version</td>\n",
       "      <td id=\"T_7b1e7_row16_col1\" class=\"data row16 col1\" >1.8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7b1e7_row17_col0\" class=\"data row17 col0\" >Data directory</td>\n",
       "      <td id=\"T_7b1e7_row17_col1\" class=\"data row17 col1\" >/tmp/fablib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7b1e7_row18_col0\" class=\"data row18 col0\" >Project Name</td>\n",
       "      <td id=\"T_7b1e7_row18_col1\" class=\"data row18 col1\" >DYNAMOS project Energy Efficiency</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7e4ba2930a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "from fabrictestbed_extensions.fablib.fablib import FablibManager as fablib_manager\n",
    "\n",
    "fablib = fablib_manager()\n",
    "\n",
    "fablib.show_config();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9cae62c-dec2-4e07-b6c5-51b651edbf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IPv4Address('10.145.1.2'), IPv4Address('10.145.1.3'), IPv4Address('10.145.1.4'), IPv4Address('10.145.1.5'), IPv4Address('10.145.1.6'), IPv4Address('10.145.1.7'), IPv4Address('10.145.1.8'), IPv4Address('10.145.1.9'), IPv4Address('10.145.1.10'), IPv4Address('10.145.1.11')] [IPv4Address('10.145.1.1')]\n"
     ]
    }
   ],
   "source": [
    "slice = fablib.get_slice(name=\"DYNAMOS-on-FABRIC\");\n",
    "nodes = slice.get_nodes();\n",
    "network = slice.get_networks()[0];\n",
    "network.config()\n",
    "subnet = network.get_subnet();\n",
    "gateway = network.get_gateway();\n",
    "allocated_ips = network.get_allocated_ips();\n",
    "available_ips = network.get_available_ips(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "542225f7-0ffe-4035-87ed-e632498bb28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IPv4Address('10.145.1.2'), IPv4Address('10.145.1.3'), IPv4Address('10.145.1.4'), IPv4Address('10.145.1.5'), IPv4Address('10.145.1.6'), IPv4Address('10.145.1.7'), IPv4Address('10.145.1.8'), IPv4Address('10.145.1.9'), IPv4Address('10.145.1.10'), IPv4Address('10.145.1.11')] [IPv4Address('10.145.1.1')]\n"
     ]
    }
   ],
   "source": [
    "print(available_ips, allocated_ips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e2286c8-2bef-480b-afab-d1b6755071fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10.145.1.2', '10.145.1.3', '10.145.1.4']\n"
     ]
    }
   ],
   "source": [
    "def get_ip(node):\n",
    "    interface = node.get_interface(network_name=\"NET1\")\n",
    "    return interface.get_ip_addr()\n",
    "\n",
    "ips = [get_ip(node) for node in nodes];\n",
    "\n",
    "print(ips);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0daeae51-14f8-44b4-8c67-b29bc73bc296",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory = (\n",
    "    f\"[kube_control_plane]\\n\"\n",
    "    f\"node1 ansible_host={ips[0]} ip={ips[0]} etcd_member_name=etcd1\\n\"\n",
    "    f\"\\n\"\n",
    "    f\"[etcd:children]\\n\"\n",
    "    f\"kube_control_plane\\n\"\n",
    "    f\"\\n\"\n",
    "    f\"[kube_node]\\n\"\n",
    "    f\"node2 ansible_host=10.145.5.3 ip=10.145.5.3\\n\"\n",
    "    f\"node3 ansible_host=10.145.5.4 ip=10.145.5.4\\n\"\n",
    ")\n",
    "\n",
    "for i, ip in enumerate(ips):\n",
    "    inventory + f\"node{i + 1} ansoble_host={ip} ip={ip}\\n\"\n",
    "\n",
    "with open('kubespray/inventory.ini', 'w') as f:\n",
    "    f.write(inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ab32ff9-8d2b-453f-a6e1-eecfd94920c1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.145.2.0/24 10.145.2.2\n",
      "W0404 11:07:05.312663   18446 preflight.go:56] [reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.\n",
      "[reset] Are you sure you want to proceed? [y/N]: [preflight] Running pre-flight checks\n",
      "W0404 11:07:05.312709   18446 removeetcdmember.go:106] [reset] No kubeadm config, using etcd pod spec to get data directory\n",
      "[reset] Deleted contents of the etcd data directory: /var/lib/etcd\n",
      "[reset] Stopping the kubelet service\n",
      "[reset] Unmounting mounted directories in \"/var/lib/kubelet\"\n",
      "[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki]\n",
      "[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]\n",
      "\n",
      "The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d\n",
      "\n",
      "The reset process does not reset or clean up iptables rules or IPVS tables.\n",
      "If you wish to reset iptables, you must do so manually by using the \"iptables\" command.\n",
      "\n",
      "If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)\n",
      "to reset your system's IPVS tables.\n",
      "\n",
      "The reset process does not clean your kubeconfig files and you must remove them manually.\n",
      "Please, check the contents of the $HOME/.kube/config file.\n",
      "I0404 11:07:06.262373   18457 version.go:256] remote version is much newer: v1.32.3; falling back to: stable-1.30\n",
      "[init] Using Kubernetes version: v1.30.11\n",
      "[preflight] Running pre-flight checks\n",
      "[preflight] Pulling images required for setting up a Kubernetes cluster\n",
      "[preflight] This might take a minute or two, depending on the speed of your internet connection\n",
      "[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\n",
      "W0404 11:07:06.900706   18457 checks.go:844] detected that the sandbox image \"registry.k8s.io/pause:3.8\" of the container runtime is inconsistent with that used by kubeadm.It is recommended to use \"registry.k8s.io/pause:3.9\" as the CRI sandbox image.\n",
      "[certs] Using certificateDir folder \"/etc/kubernetes/pki\"\n",
      "[certs] Generating \"ca\" certificate and key\n",
      "[certs] Generating \"apiserver\" certificate and key\n",
      "[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local node1] and IPs [10.96.0.1 10.145.2.2]\n",
      "[certs] Generating \"apiserver-kubelet-client\" certificate and key\n",
      "[certs] Generating \"front-proxy-ca\" certificate and key\n",
      "[certs] Generating \"front-proxy-client\" certificate and key\n",
      "[certs] Generating \"etcd/ca\" certificate and key\n",
      "[certs] Generating \"etcd/server\" certificate and key\n",
      "[certs] etcd/server serving cert is signed for DNS names [localhost node1] and IPs [10.145.2.2 127.0.0.1 ::1]\n",
      "[certs] Generating \"etcd/peer\" certificate and key\n",
      "[certs] etcd/peer serving cert is signed for DNS names [localhost node1] and IPs [10.145.2.2 127.0.0.1 ::1]\n",
      "[certs] Generating \"etcd/healthcheck-client\" certificate and key\n",
      "[certs] Generating \"apiserver-etcd-client\" certificate and key\n",
      "[certs] Generating \"sa\" key and public key\n",
      "[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"\n",
      "[kubeconfig] Writing \"admin.conf\" kubeconfig file\n",
      "[kubeconfig] Writing \"super-admin.conf\" kubeconfig file\n",
      "[kubeconfig] Writing \"kubelet.conf\" kubeconfig file\n",
      "[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file\n",
      "[kubeconfig] Writing \"scheduler.conf\" kubeconfig file\n",
      "[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"\n",
      "[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"\n",
      "[control-plane] Creating static Pod manifest for \"kube-apiserver\"\n",
      "[control-plane] Creating static Pod manifest for \"kube-controller-manager\"\n",
      "[control-plane] Creating static Pod manifest for \"kube-scheduler\"\n",
      "[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n",
      "[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n",
      "[kubelet-start] Starting the kubelet\n",
      "[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\"\n",
      "[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s\n",
      "[kubelet-check] The kubelet is healthy after 501.485225ms\n",
      "[api-check] Waiting for a healthy API server. This can take up to 4m0s\n",
      "[api-check] The API server is healthy after 6.501864665s\n",
      "[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n",
      "[kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster\n",
      "[upload-certs] Skipping phase. Please see --upload-certs\n",
      "[mark-control-plane] Marking the node node1 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]\n",
      "[mark-control-plane] Marking the node node1 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]\n",
      "[bootstrap-token] Using token: k79q8q.xwsob4tqgx89que3\n",
      "[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\n",
      "[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes\n",
      "[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n",
      "[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n",
      "[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n",
      "[bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n",
      "[kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key\n",
      "[addons] Applied essential addon: CoreDNS\n",
      "[addons] Applied essential addon: kube-proxy\n",
      "\n",
      "Your Kubernetes control-plane has initialized successfully!\n",
      "\n",
      "To start using your cluster, you need to run the following as a regular user:\n",
      "\n",
      "  mkdir -p $HOME/.kube\n",
      "  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n",
      "  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n",
      "\n",
      "Alternatively, if you are the root user, you can run:\n",
      "\n",
      "  export KUBECONFIG=/etc/kubernetes/admin.conf\n",
      "\n",
      "You should now deploy a pod network to the cluster.\n",
      "Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n",
      "  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n",
      "\n",
      "Then you can join any number of worker nodes by running the following on each as root:\n",
      "\n",
      "kubeadm join 10.145.2.2:6443 --token k79q8q.xwsob4tqgx89que3 \\\n",
      "\t--discovery-token-ca-cert-hash sha256:2348c969b60ddeffcde2ccac0cb4acac1a28e44a539e463e0b8910ce3f9c46b3 \n",
      "poddisruptionbudget.policy/calico-kube-controllers created\n",
      "serviceaccount/calico-kube-controllers created\n",
      "serviceaccount/calico-node created\n",
      "configmap/calico-config created\n",
      "customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created\n",
      "customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created\n",
      "clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created\n",
      "clusterrole.rbac.authorization.k8s.io/calico-node created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/calico-node created\n",
      "daemonset.apps/calico-node created\n",
      "deployment.apps/calico-kube-controllers created\n",
      "NAME    STATUS     ROLES           AGE   VERSION\n",
      "node1   NotReady   control-plane   37s   v1.30.11\n"
     ]
    }
   ],
   "source": [
    "nodes[0].upload_file(local_file_path=\"node_scripts/control_kubespray_setup.sh\", remote_file_path=\"kubespray_setup.sh\")\n",
    "nodes[0].upload_file(local_file_path=\"kubespray/inventory.ini\", remote_file_path=\"kubespray/inventory/dynamos/inventory.ini\")\n",
    "nodes[0].upload_file(local_file_path=\"kubespray/ansible.cfg\", remote_file_path=\"kubespray/ansible.cfg\")\n",
    "nodes[0].execute(f\"chmod +x kubespray_setup.sh && ./kubespray_setup.sh\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86323e2-bfab-4cf4-995b-ddbff2465458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting node 1\n",
      "10.30.6.60\n",
      "W0404 08:24:08.749532   28972 preflight.go:56] [reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.\n",
      "[reset] Are you sure you want to proceed? [y/N]: [preflight] Running pre-flight checks\n",
      "W0404 08:24:08.749619   28972 removeetcdmember.go:106] [reset] No kubeadm config, using etcd pod spec to get data directory\n",
      "[reset] Deleted contents of the etcd data directory: /var/lib/etcd\n",
      "[reset] Stopping the kubelet service\n",
      "[reset] Unmounting mounted directories in \"/var/lib/kubelet\"\n",
      "[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki]\n",
      "[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]\n",
      "\n",
      "The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d\n",
      "\n",
      "The reset process does not reset or clean up iptables rules or IPVS tables.\n",
      "If you wish to reset iptables, you must do so manually by using the \"iptables\" command.\n",
      "\n",
      "If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)\n",
      "to reset your system's IPVS tables.\n",
      "\n",
      "The reset process does not clean your kubeconfig files and you must remove them manually.\n",
      "Please, check the contents of the $HOME/.kube/config file.\n",
      "[preflight] Running pre-flight checks\n"
     ]
    }
   ],
   "source": [
    "async def start_node(node):\n",
    "    print(f\"Starting node\")\n",
    "    node.upload_file(local_file_path=\"node_scripts/worker_node_start.sh\", remote_file_path=\"start.sh\")\n",
    "    node.execute(f\"chmod +x start.sh && ./start.sh {ips[0]}\");\n",
    "\n",
    "tasks = [\n",
    "    asyncio.create_task(start_node(node))\n",
    "    for node in nodes\n",
    "]\n",
    "\n",
    "for task in tasks:\n",
    "    await task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522d8c05-a2ae-45cc-a990-7e25b528593b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
